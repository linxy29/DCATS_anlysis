---
title: "debug_dcatsGLM"
author: "Xinyi Lin"
date: "6/8/2021"
output: html_document
---

I found out `dcats_GLM` always gives p-value larger than 0.05. In this file, I tried to find out why.

```{r}
library(tidyverse)
library(DCATS)
```

```{r}
dcats_GLM <- function(count_mat, design_mat, similarity_mat=NULL, n_samples=50,
                      pseudo_count=NULL,  base_model='NULL') {
  # Output matrices
  coeffs     <- matrix(NA, ncol(count_mat), ncol(design_mat))
  coeffs_err <- matrix(NA, ncol(count_mat), ncol(design_mat))
  LR_vals    <- matrix(NA, ncol(count_mat), ncol(design_mat))
  LRT_pvals  <- matrix(NA, ncol(count_mat), ncol(design_mat))
  LRT_fdr    <- matrix(NA, ncol(count_mat), ncol(design_mat))

  # Check colnames
  if (is.null(colnames(count_mat)))
    colnames(count_mat) <- paste0('cell_type_', seq(ncol(count_mat)))
  if (is.null(colnames(design_mat)))
    colnames(design_mat) <- paste0('factor_', seq(ncol(design_mat)))

  # Add rownames and colnames
  rownames(LR_vals) <- rownames(LRT_pvals) <- rownames(LRT_fdr) <-
    rownames(coeffs) <- rownames(coeffs_err) <- colnames(count_mat)
  colnames(LR_vals) <- colnames(LRT_pvals) <- colnames(LRT_fdr) <-
    colnames(coeffs) <- colnames(coeffs_err) <- colnames(design_mat)
  
  
  ## using estimated the latent cell counts
  count_latent = count_mat
  if(!is.null(similarity_mat)) {
    for (i in seq_len(nrow(count_mat))) {
      count_latent[i, ] <- sum(count_mat[i, ]) *
        multinom_EM(count_mat[i, ], similarity_mat, verbose = FALSE)$mu
    }
  }
  
  K <- ncol(count_mat) ## number of cell types
  if (is.null(similarity_mat)) {
    n_samples <- 1
  }
  
  if (!is.null(n_samples) && !is.null(similarity_mat)) {
    count_use <- matrix(0, nrow(count_mat) * n_samples, K)
    for (i in seq_len(nrow(count_mat))) {
      idx <- seq((i - 1) * n_samples + 1, i * n_samples)
      for (j in seq_len(K)) {
        count_use[idx, ] <- (
          count_use[idx, ] + t(rmultinom(n_samples, count_latent[i, j], similarity_mat[j, ])))
      }
    }
  } else{
    count_use <- count_mat
  }
  
  # adding pseudo counts
  if (is.null(pseudo_count)) {
    if (any(colMeans(count_mat) == 0)) {
      print(paste("Empty cell type exists in at least one conidtion;",
                  "adding replicate & condition specific pseudo count:"))
      count_use <- count_use + 1
    }
  } else {
    count_use = count_use + pseudo_count
  }
  
  count_use = round(count_use)
  
  # Test each factor
  for (m in seq_len(ncol(count_use))) {
    for (k in seq_len(ncol(design_mat))) {
      df_use <- data.frame(n1 = count_use[, m], total=rowSums(count_use))
      df_use <- cbind(df_use, as.data.frame(design_mat)[, k, drop=FALSE])

      df_tmp <- df_use[!is.na(design_mat[, k]), ]
      
      ## model fitting using betabin
      fm0 <- aod::betabin(cbind(n1, total-n1) ~ 1, ~ 1, data = df_tmp)
      formula_fix <- as.formula(paste0('cbind(n1, total-n1)', '~ 1+',
                                         colnames(design_mat)[k], sep=''))
      fm1 <- aod::betabin(formula_fix, ~ 1, data = df_tmp)

      ## ignore the fitting if the hessian matrix is singular
      if (length(fm1@varparam) < 4 || is.na(fm1@varparam[2, 2])) {next}

      LR_vals[m, k] <- fm0@dev - fm1@dev
      LRT_pvals[m, k] <- pchisq(LR_vals[m, k], df=1, lower.tail = FALSE, log.p = FALSE)

      coeffs[m, k] <- fm1@param[2]
      coeffs_err[m, k] <- fm1@varparam[2, 2]
    }
  }

  # Return list
  LRT_fdr[,] <- p.adjust(LRT_pvals, method = 'fdr')
  res <- list('ceoffs'=coeffs, 'coeffs_err'=coeffs_err,
              'LR_vals'=LR_vals, 'pvals'=LRT_pvals, 'fdr'=LRT_fdr)
  res
}
```

## Test 1

Test whether `dcats_GLM` gives the correct results with data not simulated from any distribution -> works

```{r}
set.seed(3)
K <- 3
totals1 = rep(300, 4)
totals2 = rep(300, 3)
prop_s1 = rep(1/K, K)
prop_s2 = c(1/3, 1/2, 1/6)
numb_cond1 = totals1 %*% t(prop_s1)
numb_cond2 = totals2 %*% t(prop_s2)
sim_count = rbind(numb_cond1, numb_cond2)
sim_design = matrix(c("g1", "g1", "g1", "g2", "g2", "g2", "g2"), ncol = 1)
dcats_GLM(sim_count, sim_design)
```

Test whether `dcats_GLM` gives the correct results with data simulated from multinomial

```{r}
set.seed(3)
K <- 3
prop_s1 = rep(1/K, K)
prop_s2 = c(1/3, 1/2, 1/6)
numb_cond1 = rmultinom(3, 300, prop_s1) %>% t()
numb_cond2 = rmultinom(4, 300, prop_s2) %>% t()
sim_count = rbind(numb_cond1, numb_cond2)
sim_design = matrix(c("g1", "g1", "g1", "g2", "g2", "g2", "g2"), ncol = 1)
dcats_GLM(sim_count, sim_design)
```

## add dirichlet

```{r}
set.seed(3)
K <- 3
concentration = 50
numb_cond1 = matrix(nrow = 3, ncol = K)
numb_cond2 = matrix(nrow = 4, ncol = K)
for (rep in 1:3) {
  prop_cond1 <- MCMCpack::rdirichlet(1, prop_s1 * concentration)
  numb_cond1[rep, ] <- rmultinom(1, 500, prop_cond1)}
for (rep in 1:4) {
  prop_cond2 <- MCMCpack::rdirichlet(1, prop_s1 * concentration)
  numb_cond2[rep, ] <- rmultinom(1, 500, prop_cond2)}
sim_count = rbind(numb_cond1, numb_cond2)
sim_design = matrix(c("g1", "g1", "g1", "g2", "g2", "g2", "g2"), ncol = 1)
dcats_GLM(sim_count, sim_design)
```

```{r}
set.seed(3)
K <- 3
totals1 = rep(300, 4)
totals2 = rep(300, 3)
diri_s1 = rep(1/K, K) * 200
diri_s2 = c(1/3, 1/2, 1/6) * 200
simil_mat = create_simMat(K, confuse_rate=0.1)
sim_dat <- DCATS::simulator_base(totals1, totals2, diri_s1, diri_s2, simil_mat)
sim_count = rbind(sim_dat$numb_cond1, sim_dat$numb_cond2)
sim_design = matrix(c("g1", "g1", "g1", "g2", "g2", "g2", "g2"), ncol = 1)
dcats_GLM(sim_count, sim_design)
```

```{r}
diri_s1 = rep(1/K, K) * 200
diri_s2 = c(1/3, 1/2, 1/6) * 200
```

## use real-world data

```{r}
#pathSPC = "/storage/holab/linxy"  ## for server
pathSPC = "D:/Data"
```

```{r}
Kaufmann2021 = read.delim(gzfile(str_c(pathSPC, "/DCATS/Kaufmann2021_metadata_per_cell.csv.gz")), sep = ",") %>% 
  janitor::clean_names() %>% 
  mutate_if(is.character, as.factor)
head(Kaufmann2021)
summary(Kaufmann2021)
Kaufmann2021 %>% group_by(group) %>% 
  summarise(donor = unique(donor)) %>% 
  summarise(n = n())
```

## treated vs untreated

```{r}
numb_ms1Nat = Kaufmann2021 %>% 
  filter(group == "MS1_nat") %>% 
  group_by(donor, cluster_names) %>% 
  summarise(n = n()) %>% 
  pivot_wider(names_from = cluster_names, values_from = n) %>% 
  replace(is.na(.), 0) %>% 
  column_to_rownames("donor") %>% 
  as.matrix()
numb_ms1 = Kaufmann2021 %>% 
  filter(group == "MS1") %>% 
  group_by(donor, cluster_names) %>% 
  summarise(n = n()) %>% 
  pivot_wider(names_from = cluster_names, values_from = n) %>% 
  replace(is.na(.), 0) %>% 
  column_to_rownames("donor") %>% 
  as.matrix()
print(numb_ms1)
print(numb_ms1Nat)
simil_mat = create_simMat(dim(numb_ms1)[2], confuse_rate=0.1)
```

```{r}
set.seed(123)
dcats_betabin(numb_ms1Nat, numb_ms1, similarity_mat = simil_mat) %>% 
  mutate(wether_sig = ifelse(pvals < 0.05, "P", "N")) %>% 
  select(pvals, wether_sig)
```

```{r}
set.seed(123)
count_mat = rbind(numb_ms1Nat, numb_ms1)
design_mat = c(rep("ms1Nat", dim(numb_ms1Nat)[1]), rep("ms1", dim(numb_ms1)[1])) %>% as.matrix(ncol = 1)
res_GLM = dcats_GLM_rw(count_mat = count_mat, design_mat = design_mat, similarity_mat = simil_mat)
ifelse(res_GLM$pvals < 0.05, "P", "N") 
```

```{r}
load("./temp/count_use.RData")
counts1_use = count_use[1:500, ]
counts2_use = count_use[501:950, ]
```


```{r}
dcats_betabin <- function(counts1, counts2, similarity_mat=NULL, n_samples=50,
                          pseudo_count=NULL) {
    ## Check counts1 and counts2 shape
    if (length(counts1) == 1 || is.null(dim(counts1)) ||
        length(dim(counts1)) < 2) {
        counts1 = matrix(counts1, nrow=1)
    }
    if (length(counts2) == 1 || is.null(dim(counts2)) ||
        length(dim(counts2)) < 2) {
        counts2 = matrix(counts2, nrow=1)
    }

    prop1 <- counts1 / rowSums(counts1)
    prop2 <- counts2 / rowSums(counts2)

    ## using estimated the latent cell counts
    counts1_latent = counts1
    counts2_latent = counts2
    if(!is.null(similarity_mat)) {
        for (i in seq_len(nrow(counts1))) {
            counts1_latent[i, ] <- sum(counts1[i, ]) *
                multinom_EM(counts1[i, ], similarity_mat, verbose = FALSE)$mu
        }
        for (i in seq_len(nrow(counts2))) {
            counts2_latent[i, ] <- sum(counts2[i, ]) *
                multinom_EM(counts2[i, ], similarity_mat, verbose = FALSE)$mu
        }
    }
    
    ## number of cell types
    K <- ncol(counts1)
    if (is.null(similarity_mat)) {
        n_samples <- 1
    }

    if (!is.null(n_samples) && !is.null(similarity_mat)) {
        counts1_use <- matrix(0, nrow(counts1) * n_samples, K)
        counts2_use <- matrix(0, nrow(counts2) * n_samples, K)
        for (i in seq_len(nrow(counts1))) {
            idx <- seq((i - 1) * n_samples + 1, i * n_samples)
            for (j in seq_len(K)) {
                counts1_use[idx, ] <- (
                    counts1_use[idx, ] + t(rmultinom(n_samples,
                                                     counts1_latent[i, j],
                                                     similarity_mat[j, ])))
            }
        }
        for (i in seq_len(nrow(counts2))) {
            idx <- seq((i - 1) * n_samples + 1, i * n_samples)
            for (j in seq_len(K)) {
                counts2_use[idx, ] <- (
                    counts2_use[idx, ] + t(rmultinom(n_samples,
                                                     counts2_latent[i, j],
                                                     similarity_mat[j, ])))
            }
        }
    } else{
        counts1_use <- counts1
        counts2_use <- counts2
    }

    # adding pseudo counts
    if (is.null(pseudo_count)) {
        if (any(colMeans(counts1) == 0) || any(colMeans(counts2) == 0) ) {
            print(paste("Empty cell type exists in at least one conidtion;",
                        "adding replicate & condition specific pseudo count:"))
            counts1_use <- counts1_use + 1
            counts2_use <- counts2_use + 1
        }
    } else {
        counts1_use = counts1_use + pseudo_count
        counts2_use = counts2_use + pseudo_count
    }

    ## Binomial regression for each sampling
    LR_val <- matrix(NA, n_samples, K)
    coeffs_val <- matrix(NA, n_samples, K)
    coeffs_err <- matrix(NA, n_samples, K)
    intercept_val <- matrix(NA, n_samples, K)
    intercept_err <- matrix(NA, n_samples, K)
    total_all <- c(rowSums(counts1_use), rowSums(counts2_use))
    label_all <- c(rep(1, nrow(counts1_use)), rep(0, nrow(counts2_use)))
    for (ir in seq_len(n_samples)) {
        idx <- seq(1, length(total_all), n_samples) + ir - 1
        for (i in seq_len(K)) {
            n1 <- c(counts1_use[, i], counts2_use[, i])[idx]
            df <- data.frame(n1 = n1, n2 = total_all[idx] - n1,
                             label = label_all[idx])
            ## betabin GLM
            fm1 <- aod::betabin(cbind(n1, n2) ~ label, ~ 1, data = df)
            if (any(is.na(fm1@varparam))) {
                print(fm1)
                print(df)
            } else {
                coeffs_err[ir, i] <-fm1@varparam[2, 2]
                }
            coeffs_val[ir, i] <- fm1@param[2]
            intercept_val[ir, i] <- fm1@param[1] # summary(fm1)@Coef[1, 1]
            intercept_err[ir, i] <- fm1@varparam[1, 1]
            
            fm0 <- aod::betabin(cbind(n1, n2) ~ 1, ~ 1, data = df)
            LR_val[ir, i] <- fm0@dev - fm1@dev
            }
    }

    ## Averaging the coeficients errors
    if (is.null(n_samples) || is.null(similarity_mat) || n_samples == 1) {
        coeff_val_mean <- colMeans(coeffs_val)
        coeff_err_pool <- colMeans(coeffs_err**2)
        intercept_val_mean <- colMeans(intercept_val)
        intercept_err_pool <- colMeans(intercept_err**2)
    } else{
        coeff_val_mean <- colMeans(coeffs_val)
        coeff_err_pool <- colMeans(coeffs_err**2) +
            matrixStats::colSds(coeffs_val) +
            matrixStats::colSds(coeffs_val) / n_samples

        intercept_val_mean <- colMeans(intercept_val)
        intercept_err_pool <- colMeans(intercept_err**2) +
            matrixStats::colSds(intercept_val) +
            matrixStats::colSds(intercept_val) / n_samples
    }

    # p values with Ward test: https://en.wikipedia.org/wiki/Wald_test
    pvals <- pnorm(-abs(coeff_val_mean) / sqrt(coeff_err_pool))  * 2

    LR_median = robustbase::colMedians(LR_val)
    LRT_pvals <-  pchisq(LR_median, df=1, lower.tail = FALSE, log.p = FALSE)

    data.frame(
        "prop1_mean" = colMeans(prop1),
        "prop1_std"  = matrixStats::colSds(prop1),
        "prop2_mean" = colMeans(prop2),
        "prop2_std"  = matrixStats::colSds(prop2),
        "coeff_mean" = coeff_val_mean,
        "coeff_std"  = sqrt(coeff_err_pool),
        "intecept_mean" = intercept_val_mean,
        "intecept_std"  = sqrt(intercept_err_pool),
        "pvals" = pvals,
        "LRT_pvals" = LRT_pvals,
        row.names = colnames(counts1)
    )
}
```

```{r}
dcats_GLM_rw <- function(count_mat, design_mat, similarity_mat=NULL, n_samples=50,pseudo_count=NULL,  base_model='NULL') {
  # Output matrices
  coeffs     <- matrix(NA, ncol(count_mat), ncol(design_mat))
  coeffs_err <- matrix(NA, ncol(count_mat), ncol(design_mat))
  LR_vals    <- matrix(NA, ncol(count_mat), ncol(design_mat))
  LRT_pvals  <- matrix(NA, ncol(count_mat), ncol(design_mat))
  LRT_fdr    <- matrix(NA, ncol(count_mat), ncol(design_mat))

  # Check colnames
  if (is.null(colnames(count_mat)))
    colnames(count_mat) <- paste0('cell_type_', seq(ncol(count_mat)))
  if (is.null(colnames(design_mat)))
    colnames(design_mat) <- paste0('factor_', seq(ncol(design_mat)))

  # Add rownames and colnames
  rownames(LR_vals) <- rownames(LRT_pvals) <- rownames(LRT_fdr) <-
    rownames(coeffs) <- rownames(coeffs_err) <- colnames(count_mat)
  colnames(LR_vals) <- colnames(LRT_pvals) <- colnames(LRT_fdr) <-
    colnames(coeffs) <- colnames(coeffs_err) <- colnames(design_mat)
  
  
  ## using estimated the latent cell counts
  count_latent = count_mat
  if(!is.null(similarity_mat)) {
    for (i in seq_len(nrow(count_mat))) {
      count_latent[i, ] <- sum(count_mat[i, ]) *
        multinom_EM(count_mat[i, ], similarity_mat, verbose = FALSE)$mu
    }
  }
  
  K <- ncol(count_mat) ## number of cell types
  if (is.null(similarity_mat)) {
    n_samples <- 1
  }
  
  if (!is.null(n_samples) && !is.null(similarity_mat)) {
    count_use <- matrix(0, nrow(count_mat) * n_samples, K)
    for (i in seq_len(nrow(count_mat))) {
      idx <- seq((i - 1) * n_samples + 1, i * n_samples)
      for (j in seq_len(K)) {
        count_use[idx, ] <- (
          count_use[idx, ] + t(rmultinom(n_samples, count_latent[i, j], similarity_mat[j, ])))
      }
    }
  } else{
    count_use <- count_mat
  }
  
  # adding pseudo counts
  if (is.null(pseudo_count)) {
    if (any(colMeans(count_mat) == 0)) {
      print(paste("Empty cell type exists in at least one conidtion;",
                  "adding replicate & condition specific pseudo count:"))
      count_use <- count_use + 1
    }
  } else {
    count_use = count_use + pseudo_count
  }
  
  count_use = round(count_use)
  
  # Test each factor
  for (k in seq_len(ncol(design_mat))) {    ## for each factor
    sub_LR_val <- matrix(NA, n_samples, K)
    sub_coeffs_val <- matrix(NA, n_samples, K)
    sub_coeffs_err <- matrix(NA, n_samples, K)
    for (ir in seq_len(n_samples)) {          ## for each sampling
      for (m in seq_len(ncol(count_use))) {       ## for each cluster
        idx <- seq(1, nrow(count_use), n_samples) + ir - 1
        
        df_use <- data.frame(n1 = count_use[, m], total=rowSums(count_use))
        df_use <- cbind(df_use, as.data.frame(design_mat)[, k, drop=FALSE])[idx,]
        df_tmp <- df_use[!is.na(design_mat[, k]), ]
      
      ## model fitting using betabin
      fm0 <- aod::betabin(cbind(n1, total-n1) ~ 1, ~ 1, data = df_tmp)
      #formula_fix <- as.formula(paste0('cbind(n1, total-n1)', '~ 1+', colnames(design_mat)[k], sep=''))
      formula_fix <- as.formula(paste0('cbind(n1, total-n1)', '~ ', colnames(design_mat)[k], sep=''))
      fm1 <- aod::betabin(formula_fix, ~ 1, data = df_tmp)

      ## ignore the fitting if the hessian matrix is singular
      if (length(fm1@varparam) < 4 || is.na(fm1@varparam[2, 2])) {next}
      
      sub_LR_val[ir, m] <- fm0@dev - fm1@dev
      sub_coeffs_val[ir, m] <- fm1@param[2]
      sub_coeffs_err[ir, m] <-fm1@varparam[2, 2]
      }
    }
      ## averaging the estimation to get the final result
      if (is.null(n_samples) || is.null(similarity_mat) || n_samples == 1){
        sub_coeff_err_pool <- colMeans(sub_coeffs_err**2, na.rm = TRUE)
      } else {
        sub_coeff_err_pool <- colMeans(sub_coeffs_err**2, na.rm = TRUE) +
            matrixStats::colSds(sub_coeffs_val) +
            matrixStats::colSds(sub_coeffs_val) / n_samples
      }
      LR_median = robustbase::colMedians(sub_LR_val, na.rm = TRUE)
      
      LR_vals[, k] <- LR_median
      LRT_pvals[, k] <- pchisq(LR_median, df=1, lower.tail = FALSE, log.p = FALSE)
      coeffs[, k] <- colMeans(sub_coeffs_val, na.rm = TRUE)
      coeffs_err[, k] <- sqrt(sub_coeff_err_pool)
  }

  # Return list
  LRT_fdr[,] <- p.adjust(LRT_pvals, method = 'fdr')
  res <- list('ceoffs'=coeffs, 'coeffs_err'=coeffs_err,
              'LR_vals'=LR_vals, 'pvals'=LRT_pvals, 'fdr'=LRT_fdr)
  res
}
```